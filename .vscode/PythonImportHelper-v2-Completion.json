[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "openllm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openllm",
        "description": "openllm",
        "detail": "openllm",
        "documentation": {}
    },
    {
        "label": "runpod",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "runpod",
        "description": "runpod",
        "detail": "runpod",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "adjust_concurrency",
        "kind": 2,
        "importPath": "handler",
        "description": "handler",
        "peekOfCode": "def adjust_concurrency(current_concurrency):\n    return concurrency_modifier\nif mode_to_run in [\"both\", \"serverless\"]:\n    runpod.serverless.start({\n        \"handler\": handler,\n        \"concurrency_modifier\": adjust_concurrency,\n        \"return_aggregate_stream\": True,\n    })\nif mode_to_run == \"pod\":\n    async def main():",
        "detail": "handler",
        "documentation": {}
    },
    {
        "label": "model_name",
        "kind": 5,
        "importPath": "handler",
        "description": "handler",
        "peekOfCode": "model_name = os.getenv(\"MODEL\", \"mistralai/Mistral-7B-Instruct-v0.1\")\nconcurrency_modifier = int(os.getenv(\"CONCURRENCY_MODIFIER\", 1))\nmode_to_run = os.getenv(\"MODE_TO_RUN\", \"pod\")\nmodel_length_default = 25000\nprint(\"------- ENVIRONMENT VARIABLES -------\")\nprint(\"Concurrency: \", concurrency_modifier)\nprint(\"Mode running: \", mode_to_run)\nprint(\"------- -------------------- -------\")\n# LOAD THE MODEL\n# model = loadModelPseudo(model_name)",
        "detail": "handler",
        "documentation": {}
    },
    {
        "label": "concurrency_modifier",
        "kind": 5,
        "importPath": "handler",
        "description": "handler",
        "peekOfCode": "concurrency_modifier = int(os.getenv(\"CONCURRENCY_MODIFIER\", 1))\nmode_to_run = os.getenv(\"MODE_TO_RUN\", \"pod\")\nmodel_length_default = 25000\nprint(\"------- ENVIRONMENT VARIABLES -------\")\nprint(\"Concurrency: \", concurrency_modifier)\nprint(\"Mode running: \", mode_to_run)\nprint(\"------- -------------------- -------\")\n# LOAD THE MODEL\n# model = loadModelPseudo(model_name)\nasync def handler(event):",
        "detail": "handler",
        "documentation": {}
    },
    {
        "label": "mode_to_run",
        "kind": 5,
        "importPath": "handler",
        "description": "handler",
        "peekOfCode": "mode_to_run = os.getenv(\"MODE_TO_RUN\", \"pod\")\nmodel_length_default = 25000\nprint(\"------- ENVIRONMENT VARIABLES -------\")\nprint(\"Concurrency: \", concurrency_modifier)\nprint(\"Mode running: \", mode_to_run)\nprint(\"------- -------------------- -------\")\n# LOAD THE MODEL\n# model = loadModelPseudo(model_name)\nasync def handler(event):\n    inputReq = event.get(\"input\", {})",
        "detail": "handler",
        "documentation": {}
    },
    {
        "label": "model_length_default",
        "kind": 5,
        "importPath": "handler",
        "description": "handler",
        "peekOfCode": "model_length_default = 25000\nprint(\"------- ENVIRONMENT VARIABLES -------\")\nprint(\"Concurrency: \", concurrency_modifier)\nprint(\"Mode running: \", mode_to_run)\nprint(\"------- -------------------- -------\")\n# LOAD THE MODEL\n# model = loadModelPseudo(model_name)\nasync def handler(event):\n    inputReq = event.get(\"input\", {})\n    return inputReq",
        "detail": "handler",
        "documentation": {}
    },
    {
        "label": "submit_job_to_runpod",
        "kind": 2,
        "importPath": "stream_client_side",
        "description": "stream_client_side",
        "peekOfCode": "def submit_job_to_runpod(api_key, server_endpoint_id, prompt, answer_type=\"normal\"):\n    \"\"\"\n    Submits a job to a specific Runpod serverless endpoint.\n    Args:\n        api_key (str): Runpod API key for authentication.\n        server_endpoint_id (str): The specific endpoint ID for the serverless function on Runpod.\n        prompt (str): The text prompt for the serverless function to process.\n        answer_type (str): Specifies how the answer should be returned ('normal' or 'stream').\n    Returns:\n        dict: Direct response from the serverless function or job ID if asynchronous.",
        "detail": "stream_client_side",
        "documentation": {}
    },
    {
        "label": "submit_job_and_stream_output",
        "kind": 2,
        "importPath": "stream_client_side",
        "description": "stream_client_side",
        "peekOfCode": "def submit_job_and_stream_output(api_key, endpoint_id, prompt):\n    \"\"\"\n    Submits a job to the specified endpoint and streams the output as it becomes available.\n    Args:\n        api_key (str): The API key for authentication.\n        endpoint_id (str): The endpoint ID for the Runpod function.\n        prompt (str): The text prompt for the serverless function to process.\n    Returns:\n        None: Directly prints streamed outputs to the console.\n    \"\"\"",
        "detail": "stream_client_side",
        "documentation": {}
    },
    {
        "label": "check_job_status",
        "kind": 2,
        "importPath": "stream_client_side",
        "description": "stream_client_side",
        "peekOfCode": "def check_job_status(api_key, server_endpoint_id, job_id, poll=False, polling_interval=20):\n    \"\"\"\n    Checks the status of a job on Runpod.\n    Args:\n        api_key (str): Runpod API key for authentication.\n        server_endpoint_id (str): The specific endpoint ID for the serverless function on Runpod.\n        job_id (str): The job ID to check the status for.\n        poll (bool, optional): If True, continue to check the status at intervals until the job is completed. Defaults to False.\n        polling_interval (int, optional): Time in seconds to wait between status checks if polling. Defaults to 20 seconds.\n    Returns:",
        "detail": "stream_client_side",
        "documentation": {}
    }
]